[
  {
    "objectID": "InstrumentLayout.html",
    "href": "InstrumentLayout.html",
    "title": "Dashboard Overview",
    "section": "",
    "text": "Having processed your Instrument QC data for that instrument, when you check the data/instrument/archive folder you will now find both archive data .csv files corresponding to the data for both Gain and MFI tracking. We are now ready to discuss how the dashboard elements are coded and assemble to produce the individual webpages.\nFor our overall dashboard layout, the home/summary page is specified by the code contained within the index.qmd file. It’s main purpose is to serve as a landing page that rapidly loads in (since it doesn’t have any interactive elements).\nThen, each instrument has its own interactive webpage for exploring its data (taking longer to load as the data exploration is fully interactive). Each instruments webpage is specified by code contained within the individual instrument .qmd files (Aurora3L.qmd, Aurora4L.qmd, Aurora5L.qmd, AuroraCS.qmd). These share similar code layouts, but mainly differ in that 1) the file paths reference different archive data folders corresponding to their respective instruments; and 2) given each instrument has different laser-configurations, those with UV and Yellow-Green Lasers will have more plot elements than the others.\nFor this walk-though, we will be examining the the Aurora5L.qmd file from the original repository."
  },
  {
    "objectID": "InstrumentLayout.html#retrieving-data",
    "href": "InstrumentLayout.html#retrieving-data",
    "title": "Dashboard Overview",
    "section": "Retrieving Data",
    "text": "Retrieving Data\nHaving set up the file.path (MainFolder) and specified the particular instrument folder (“5L” in this case) the next two code blocks retrieve the data in the Gain and MFI .csv files, and then filter the data for the last twelve months. If you wanted to modify the time period shown, you would edit the code block at this point to increase/decrease the range.\n\nMFI_5L &lt;- Luciernaga:::CurrentData(x=\"5L\", MainFolder=MainFolder, type = \"MFI\")\nGain_5L &lt;- Luciernaga:::CurrentData(x=\"5L\", MainFolder=MainFolder, type = \"Gain\")\n\n\nWindowOfInterest &lt;- Sys.time() - months(12)\n\nMFI_5L &lt;- MFI_5L %&gt;% filter(DateTime &gt;= WindowOfInterest)\nGain_5L &lt;- Gain_5L %&gt;% filter(DateTime &gt;= WindowOfInterest)\n\nThe following code block references the .csv file containing Field-Service Engineer Visits, which are depicted as red vertical dashed lines in the .pdf version of the plots that can be exported.\n\nData &lt;- read.csv(\"AuroraMaintenance.csv\", check.names=FALSE)\n\nData &lt;- Data %&gt;% filter(!str_detect(reason, \"lean\"))\n\nRepair5L &lt;- Data %&gt;% filter(instrument %in% \"5L\")"
  },
  {
    "objectID": "InstrumentLayout.html#processing-the-data",
    "href": "InstrumentLayout.html#processing-the-data",
    "title": "Dashboard Overview",
    "section": "Processing the Data",
    "text": "Processing the Data\nThe next three code chunks are rather large, each representing three types of data that will make up the three columns seen on each instruments page (MFI, Gain, RCV). We will work through the first code chunk, which is MFI.\n\nx &lt;- MFI_5L\nx &lt;- x %&gt;% dplyr::filter(Timepoint %in% c(\"Before\", \"After\"))\nTheColumns &lt;- x %&gt;% select(where(~is.numeric(.)||is.integer(.))) %&gt;% colnames()\nTheColumns &lt;- setdiff(TheColumns, \"TIME\")\nTheIntermediate &lt;- TheColumns[!str_detect(TheColumns, \"Gain\")]\nTheColumnNames &lt;- TheIntermediate[str_detect(TheIntermediate, \"-A\")]\n  \nUltraVioletGains &lt;- TheColumnNames[str_detect(TheColumnNames, \"^UV\")]\nVioletGains &lt;- TheColumnNames[str_detect(TheColumnNames, \"^V\")]\nBlueGains &lt;- TheColumnNames[str_detect(TheColumnNames, \"^B\")]\nYellowGreenGains &lt;- TheColumnNames[str_detect(TheColumnNames, \"^YG\")]\nRedGains &lt;- TheColumnNames[str_detect(TheColumnNames, \"^R\")]\n\nScatterGains &lt;- TheIntermediate[str_detect(TheIntermediate, \"SC-\")]\nScatterGains &lt;- Luciernaga:::ScalePriority(ScatterGains)\nLaserGains &lt;- TheIntermediate[str_detect(TheIntermediate, \"Laser\")]\nLaserGains &lt;- Luciernaga:::ColorPriority(LaserGains)\nScalingGains &lt;- TheIntermediate[str_detect(TheIntermediate, \"Scaling\")]\nScalingGains &lt;- Luciernaga:::ColorPriority(ScalingGains)\nOtherGains &lt;- c(ScatterGains, LaserGains, ScalingGains)\n\nThe retrieved data for MFI goes through a couple processing steps to retrieve the column names present within the .csv file. From there it removes those that show Gain as they will be plotted along with RCV separately. Once this is done, it filters the column names by presence of string characters in their names to separate the list of colnames into shorter list based on laser, scatter, etc.\nAt this point, each of the above element is simply smaller list of column names, that will then be plotted and visualized together. This happens in the portion of the larger code chunk shown below, with the smaller list being provided to the MeasurementType arguments.\n\nUltraVioletPlotsMFI &lt;- QC_Plots(x=x, FailedFlag=TRUE, MeasurementType=UltraVioletGains,\n                      plotType = \"comparison\", returntype = \"plots\",\n                      Metadata=\"Timepoint\", strict = TRUE, YAxisLabel = \"MFI\",\n                      RepairVisits=Repair5L)\n\nVioletPlotsMFI &lt;- QC_Plots(x=x, FailedFlag=TRUE, MeasurementType=VioletGains,\n                      plotType = \"comparison\", returntype = \"plots\",\n                      Metadata=\"Timepoint\", strict = TRUE, YAxisLabel = \"MFI\",\n                      RepairVisits=Repair5L)\n\nBluePlotsMFI &lt;- QC_Plots(x=x, FailedFlag=TRUE, MeasurementType=BlueGains,\n                      plotType = \"comparison\", returntype = \"plots\",\n                      Metadata=\"Timepoint\", strict = TRUE, YAxisLabel = \"MFI\",\n                      RepairVisits=Repair5L)\n\nYellowGreenPlotsMFI &lt;- QC_Plots(x=x, FailedFlag=TRUE, MeasurementType=YellowGreenGains,\n                      plotType = \"comparison\", returntype = \"plots\",\n                      Metadata=\"Timepoint\", strict = TRUE, YAxisLabel = \"MFI\",\n                      RepairVisits=Repair5L)\n\nRedPlotsMFI &lt;- QC_Plots(x=x, FailedFlag=TRUE, MeasurementType=RedGains,\n                     plotType = \"comparison\", returntype = \"plots\",\n                     Metadata=\"Timepoint\", strict = TRUE, YAxisLabel = \"MFI\",\n                     RepairVisits=Repair5L)\n\nScatterPlotsMFI &lt;- QC_Plots(x=x, FailedFlag=TRUE, MeasurementType=ScatterGains,\n                     plotType = \"comparison\", returntype = \"plots\",\n                     Metadata=\"Timepoint\", strict = TRUE, YAxisLabel = \" \",\n                     RepairVisits=Repair5L)\n\nLaserPlotsMFI &lt;- QC_Plots(x=x, FailedFlag=TRUE, MeasurementType=LaserGains,\n                     plotType = \"comparison\", returntype = \"plots\",\n                     Metadata=\"Timepoint\", strict = TRUE, YAxisLabel = \" \",\n                     RepairVisits=Repair5L)\n\nOnce this is done, each of the elements above contains a list of plots corresponding to the column names that were provided in the smaller list. These will be referenced later when building the dashboard in the desired layout.\nThis process is then repeated again in two large code chunks for both Gain and RCV, with some small differences in how the column names are separated into smaller list, and how the individual ggplots are generated.\nStarting on line 205 of Aurora5L.qmd we have the following code-chunk:\n\nPDFPlots &lt;- c(UltraVioletPlotsMFI, VioletPlotsMFI, BluePlotsMFI, YellowGreenPlotsMFI, RedPlotsMFI, LaserPlotsMFI, ScatterPlotsMFI, UltraVioletPlotsGain, VioletPlotsGain, BluePlotsGain, YellowGreenPlotsGain, RedPlotsGain, ScatterPlotsGain, LaserDelayPlotsGain, LaserPowerPlotsGain,  ScalingPlotsGain, UltraVioletPlotsRCV, VioletPlotsRCV, BluePlotsRCV, YellowGreenPlotsRCV, RedPlotsRCV, ScatterPlotsRCV)\n\nFilename &lt;- paste0(\"QCPlots_5L\")\n\nPDF &lt;- Utility_Patchwork(x=PDFPlots, filename=Filename, returntype=\"pdf\", outfolder=MainFolder, thecolumns=1)\n\nThis code chunk above assembled all the plots we generated in the section above, and saves them as the .pdf for the individual instrument that can be seen under the data tab of the dashboard website."
  },
  {
    "objectID": "InstrumentLayout.html#visualizing-the-data",
    "href": "InstrumentLayout.html#visualizing-the-data",
    "title": "Dashboard Overview",
    "section": "Visualizing the Data",
    "text": "Visualizing the Data\n\nMFI\nNow that the instrument data has been assembled into plots for the individual measurement types and lasers, it is time to display the plots in a way that produces the instrument page visible on the original dashboard website.\nAs mentioned at the beginning of this page, the orientation of this dashboard is set to columns. Consequently, on line 218 you will see the following “## MFI {.tabset}”. The two # designate the first column. All plots visualized until the next ## (“## Gain {.tabset}” in our case at line 345) will consequently be present within this first column.\nIf we desired to change the ordering MFI column last rather and first, we would move everything from “## MFI {.tabset}” until “## Gain {.tabset}” and shift to desired order position.\nThe presence of “{.tabset}” designates that within this colum (denoted by the ##), there will be multiple tab options to switch between. The first is seen in the following code-chunk:\n\nggplotly(UltraVioletPlotsMFI[[1]])\nggplotly(UltraVioletPlotsMFI[[2]])\n# etc...\n\nThe tab name visible on the website is denoted by the title: argument. Within the code block itself, the individual plots are made interactive using the plotly function ggplotly, calling the specific detector plot sequentially.\nThis is then repeated on the next tabs for Violet, Blue, Yellow-Green and Red detectors. Then Scatter, LaserPower, LaserDelay, LaserScatter plots are visualized in their respective tabs.\nFinally, marking the end of the first column we find a card element “{.card title=”MFI”}” that appears within the tabset at the end but contains no plots to serve as a reference of what the first column plots are showing.\n\n\nGain and rCV\nAbove we walked through the process of displaying the MFI plots generated in the first section within tabsets for individual lasers. At line 345, we encounter “## Gain {.tabset}” which designates the second column of the dashboard, containing tabs for the Gain plots by laser. The overall layout is similar to what we encountered, differing here and there based on additional plots present within it’s respective archive .csv but not found in the bead csv MFI derrived from.\nAfter all the Gain plots are plotted, we encounter the third column “## RCV {.tabset}” at line 472, continuing until the end."
  },
  {
    "objectID": "InstrumentLayout.html#summary",
    "href": "InstrumentLayout.html#summary",
    "title": "Dashboard Overview",
    "section": "Summary",
    "text": "Summary\nThe individual instrument .qmd file is setup to retrieve the instrument specific archive data and filter for a desired time range. Once this is done, the individual column names in the .csv are identified and split on the basis of characters in their names into smaller list (typically by laser). This process is then repeated for each measurement type (MFI, Gain, RCV). For the assembly of the actual dashboard itself, the ordering of the measurement type columns is denoted by the ## as encountered, with the tab-sets within each of these also displayed by the order encountered. By rearranging the order, modifications to the individual instrument dashboard pages can be customized.\nIf you have multiple instruments, you would modify each instruments .qmd file in a similar way to ensure the correct archive data folder is being referenced, and then customize which plots you want displayed when and where."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "InstrumentQC",
    "section": "",
    "text": "InstrumentQC\nHaving reached this webpage, you have likely discovered the InstrumentQC dashboard dashboard I implemented at the University of Maryland, Baltimore for our four Cytek Aurora instruments. This website is for those who would like to understand how it is implemented, either for curiosity or because you’d like to modify the code to visualize the instrument QC data for your own Cytek instruments at your current workplace.\nHaving placed significant effort into this pet-project, I would love for others in the cytometry community to benefit from it. Setting it up from scratch requires some familiarity with both R and Git, so to give the average cytometry aficionado a fighting-chance I am providing this detailed explanation of the process of setting up and customizing the dashboard.\nIf you get stuck, or parts of the documentation are unclear, please reach out by opening an issue and I will try my best to help you troubleshoot. what makes free-and-open source software great (and fun to work with in my opinion) is seeing how the collective small contributions of users improve the project over time.\nBest- David\n\n\nOverview\nAfter daily QC is run on a Cytek instrument, the relevant information is stored as .fcs and .csv files in specific folders. Using functions incorporated in the Luciernaga R package, the data associated with these newly generated files is processed using R once a day at a user-designated time (implemented via Windows Task Sceduler). The version control software Git keeps track of changes to the processed data, passing updates to the online GitHub repository for storage. This data is then referenced when building the dashboard website using Quarto, which is published as a GitHub page allowing for url access.\nFor multiple instruments, the above process is repeated on each instruments computer, and the Quarto webpages are modified to display data from each instrument."
  },
  {
    "objectID": "filepaths.html",
    "href": "filepaths.html",
    "title": "Introduction",
    "section": "",
    "text": "In the previous sections, we installed the required software, and set up the Git permissions needed for Rstudio and GitHub to communicate with each other to pass along the updated files stored locally to your remote repository.\nThe following portion focuses on the next steps occurring on the individual local computers. We will need to first install the R packages that will be needed for data processing and handling. Then we will update the file.paths within the .R and .qmd files so that R knows in which folders on your local computer to look for your Daily QC report .csv and fcs files. This editing of the file.path is necessary, as the current file paths within those files is set to find the locations as seen on the UMGCC flow core computers, not yours."
  },
  {
    "objectID": "filepaths.html#cran",
    "href": "filepaths.html#cran",
    "title": "Introduction",
    "section": "CRAN",
    "text": "CRAN\nWe will start by first installing some of the R packages we will need that are found on the CRAN repository. To do so, we will first run the following code chunk:\n\ninstall.packages(\"dplyr\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"devtools\")\ninstall.packages(\"BiocManager\")\n\nWhen installing a package, we surround the package name in quotation marks. Forgetting these is a common source of error for begginers in R. For the above, we installed: the R package dplyr, which is useful for rearranging data columns and rows; the R package ggplot2 used for many data visualizations. And finally we installed BiocManager, which is the installation manager for packages stored in the Bioconductor repositories. We will use some of the functions within in the next section."
  },
  {
    "objectID": "filepaths.html#bioconductor",
    "href": "filepaths.html#bioconductor",
    "title": "Introduction",
    "section": "Bioconductor",
    "text": "Bioconductor\nBioconductor is a repository of R packages specialized for bioinformatics. The majority of Cytometry R packages can be found here. We will begin by installing a few of them by executing the following R code below:\n\nlibrary(BiocManager)\n\nNotice, to be able to install the Bioconductor packages, we first need to load the BiocManager package into active use through the library call. Unlike the install.packages() function where we surround the R package name in quotation marks, this is not required for the library() function.\nNow that BiocManager is active, we can install packages from BiocManager by following the code chunk below:\n\ninstall(\"flowCore\")\ninstall(\"flowWorkspace\")\ninstall(\"ggcyto\")\n\nThe above packages being installed were flowCore and flowWorkspace, that provide the base infrastructure needed for working with .fcs files in R, and ggcyto, which is used for visualizing the data.\nAnd finally, we will install one package from GitHub using devtools. This package is our R package Luciernaga, where the functions to process the Daily QC data files and assemble the dashboard are stored. We are in the process of submitting it to Bioconductor, but until then, it is available for now via GitHub.\n\nlibrary(devtools)\ninstall_github(\"https://github.com/DavidRach/Luciernaga\", dependencies = TRUE)"
  },
  {
    "objectID": "filepaths.html#troubleshooting",
    "href": "filepaths.html#troubleshooting",
    "title": "Introduction",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nWhen an R package is being installed, you will see messages being displayed in the consolde below as it is setup on your computer. When installation fails, there will be troubleshooting messages displayed. Typically, an R package will fail to install due to a missing dependency, ie, another R package needs to be installed first before it can be installed. By reading the failed to message note carefully, we can identify the missing package name. From there, by searching online, we can identify whether it is a CRAN or a Bioconductor package, and then use the correct installation code shown above to install that package. Once this is done, we can attempt to install the R package that had failed to install."
  },
  {
    "objectID": "filepaths.html#summary",
    "href": "filepaths.html#summary",
    "title": "Introduction",
    "section": "Summary",
    "text": "Summary\nWith the steps above executed, you now should have the main R packages needed to run the R code needed to process the instrument QC data and assemble the Quarto dashboard. Congratulations on making progress!"
  },
  {
    "objectID": "filepaths.html#overall-project-layout",
    "href": "filepaths.html#overall-project-layout",
    "title": "Introduction",
    "section": "Overall Project Layout",
    "text": "Overall Project Layout\nBefore we go track all the places we need to swap out the file.paths at, lets talk the overall structure of the Rstudio Project. To do this we will take a close look at the forked Rstudio project. In the previous section when you created in Rstudio the New Project via Git, you saved the New Project folder to a specific location (we suggested Documents). If you navigate to your Documents folder and double click on the project, you will see something that resembles this layout:\n The .qmd files are primarily either used in the creation of the dashboard, or for manual exectuion of the processing of the data (to be discussed later). Then there are .R files that are actual scripts that process the data, either when manually told to or as scheduled by the TaskScheduler. Then you have several subfolders (data, docs, images). For this section, we will focus on data, as it is where new QC files get copied to, processed and stored, and subsequently used in assembly of the dashboard website."
  },
  {
    "objectID": "filepaths.html#local",
    "href": "filepaths.html#local",
    "title": "Introduction",
    "section": "Local",
    "text": "Local"
  },
  {
    "objectID": "filepaths.html#finding-required-file-paths",
    "href": "filepaths.html#finding-required-file-paths",
    "title": "Introduction",
    "section": "Finding Required File Paths",
    "text": "Finding Required File Paths\nLet’s now start by locating your SpectroFlo associated folders you will need to have the file.paths for when modifying the R scripts. On our computer, when the instruments were set up, these were present within the local C: drive folder\n\n\n\n\n\nYou can notice there are two SpectroFlo associated folders present here, Cytekbio and Cytekbioexport. When we retrieve a file.path, we would right click the &gt; symbol. Then we would select the copy address as text option\n When we paste, we get something like this: C:\\Cytekbio. The  designates a hierarchy of folders descending down to the target folder or file. When working in R, we can reference this by taking the individual folder names and linking them with the file.path function. For this example, it would look something like the following for C:\\Cytekbio :\n\nPathToTheFolderOfInterest &lt;- file.path(\"C:\", \"CytekbioExport\")\nPathToTheFolderOfInterest\n\n[1] \"C:/CytekbioExport\"\n\n\nOne of the reasons file.path only needs the folder names is \\ vs / will vary depending on what operating system your computer is running on (Windows, Mac or Linux). Additionally, R requires Windows paths that when we right-click copy are shown as \\ be switched to /. Using file.path is easier in the long run than needing to remember which is permitted and which one is not.\nAdditionally, extra elements can be appended to the file.path by simply including the existing path variable, and then adding the next folder in between ““. For example:\n\nNextFolderDown &lt;- file.path(PathToTheFolderOfInterest, \"Experiments\")\nNextFolderDown\n\n[1] \"C:/CytekbioExport/Experiments\"\n\n\nWe can see that this is the next folder under, so let’s double click and find the file structure.\n We can see in this case that we have folders for Experiments, FCSFiles and Setup. Additionally there are files for individual dates for SetupEngineLog, AppLoginLog and ExperimentUnmixingLog that track activity previously carried out in SpectroFlo. We will navigate to the next folder “Setup”"
  },
  {
    "objectID": "filepaths.html#daily-qc-files",
    "href": "filepaths.html#daily-qc-files",
    "title": "Introduction",
    "section": "Daily QC Files",
    "text": "Daily QC Files\n\nSetupFolderPath &lt;- file.path(NextFolderDown, \"Setup\")\nSetupFolderPath\n\n[1] \"C:/CytekbioExport/Experiments/Setup\"\n\n\n\n\n\n\n\nIt is within this folder that the DailyQC reports are stored as .csv files. Opening one, we can see their overall structure resembles something like this. The individual elements are beyond the scope of this tutorial, but the data from the majority gets extracted out and utilized for the dashboard.\nUnfortunately, the .csv does not follow a “tidy” format (having gaps in spaces and rows rather than equally filled rectangular space). Consequently, a bunch of functions are used to process the data behind the scenes until it is returned in a “tidy” format that R can work with (these can later be downloaded from the Data tab on the dashboard).\nIt’s this path (SetupFolderPath) that we will need to provide to the Rscript so that it can locate new DailyQC .csv files and copy them over to the InstrumentQC folder. Make a note of this path before we continue."
  },
  {
    "objectID": "filepaths.html#bead-fcs-files-system",
    "href": "filepaths.html#bead-fcs-files-system",
    "title": "Introduction",
    "section": "Bead FCS Files (System)",
    "text": "Bead FCS Files (System)\nIf we are relying on the .fcs files acquired during the process of DailyQC, we would navigate from this folder down one additional level.\n\nQCFCSFilePath &lt;- file.path(SetupFolderPath, \"DailyQC\")\nQCFCSFilePath\n\n[1] \"C:/CytekbioExport/Experiments/Setup/DailyQC\"\n\n\nIt is here where we find the DailyQCDataSample .fcs files.\n\n\n\n\n\n\n\n\n\n\nWe would consequently provide this file.path to the Rscript to provision the files needed to calculate the MFI parameters."
  },
  {
    "objectID": "filepaths.html#bead-fcs-files-system-1",
    "href": "filepaths.html#bead-fcs-files-system-1",
    "title": "Introduction",
    "section": "Bead FCS Files (System)",
    "text": "Bead FCS Files (System)\nAt our institution, we separately using the the same QC beads used for Daily QC a 3000 bead before and after .fcs sample to compare the changes in MFI after the QC has adjusted. Within SpectroFlo, these are acquired under the Admin account, organized within an experiment corresponding to the month. As a result, these are stored with the other .fcs files acquired by all users that exist within SpectroFlo folder while they wait to be exported as zipped folders.\nBecause this can take up quite a bit of memory space in context of a core facility, for our particular SpectroFlo setup, these folders are found under an external hard-drive.\nWe would consequently start exploring the folders (and setting up a file.path) like this:\n\nExternal &lt;- file.path(\"D:\")\nExternal\n\n[1] \"D:\"\n\n\n\n\n\n\n\n\nFCSFiles &lt;- file.path(External, \"Aurora 5 FCS_Files\")\nFCSFiles\n\n[1] \"D:/Aurora 5 FCS_Files\"\n\n\n\n\n\n\n\n\nExperiments &lt;- file.path(FCSFiles, \"Experiments\")\nExperiments\n\n[1] \"D:/Aurora 5 FCS_Files/Experiments\"\n\n\n\n\n\n\n\n\nAdmin &lt;- file.path(Experiments, \"Admin\")\nAdmin\n\n[1] \"D:/Aurora 5 FCS_Files/Experiments/Admin\"\n\n\n And finally, the important thing is to note the structure that each experiment file name takes. In our case for this instrument, the folders are set to QC_2024-11. When we are having the Rscript find the new QC files for the given day, it calls the function System.time to return the date and time. These are broken into month and day.\n\nSys.time()\n\n[1] \"2024-12-19 19:28:15 EST\"\n\n\nConsequently, at the level of this folder, it would look for a folder named “QC_2024-” with the corresponding month provided by Sys.time call. R recognizes character strings exactly in this case, so if you had a name mismatch (ex. “QC 2024-” or “QC_2024_”) it would fail to find the correct folder and search the contents within. So this is an area to be aware of and adjust the code accordingly for how you structure the name."
  },
  {
    "objectID": "Automation.html",
    "href": "Automation.html",
    "title": "Manual Transfer",
    "section": "",
    "text": "Once the file.paths to find the instruments archive data are found, and the individual instrument R script and .qmd file are updated to reflect these, it is possible to start rendering the individual instrument webpage on the local computer. But when you have multiple instruments contributing data to the dashboard home page and individual instrument pages, we need a way to ensure the data from each instrument gets transferred to GitHub to allow Quarto to render the update version. This can be done either manually, or in an automated fashion. We will explore both options\n\nManual Transfer\nWe will first navigate to the InstrumentQC folder and locate the Examples_staff.qmd file. We will duplicate it, creating a new copy that will simply be called staff.qmd. This name is set within the .gitignore file to be ignored, allowing local copies with different file paths to be present on each instrument computer without causing issues in the version control.\nOnce we open the duplicated and renamed staff.qmd file, we find a single code chunk with several lines of code. This code chunk is meant to be executed by staff after morning QC has been carried out on the instrument by clicking the green play button showing “Run Current Chunk” on the upper-left side of the code-block. Once this is done, it will run the code, find the file.path to the .R script for that individual instrument, and run it. In the process, it executes the pull from the GitHub repository, processes the new data, and then pushes the updated data to the GitHub repository.\nOnce the source command has finished executing, the option is present to introduce a Flag.csv file. This will prevent automated transfer if set-up from occuring later that day (more below).\nManual sending the data has advantages as you are already at the instrument during the Instrument QC process, and the extra minute to run the current code-chunk shouldn’t add that much extra inconvenience. The drawbacks occur in that if you or others are not comfortable with Rstudio, you need to interact with it, and not forget to send the data before an user gets on the instrument.\n\n\nAutomating Transfer\nThe second option is to set up an automated transfer of the QC data from each instrument to the GitHub repository. This is possible by using the Windows Task Scheduler, which is the same system that schedules when your computer updates run, etc. The time at which the automated transfer is carried out can be designated by the user, and setup is carried out through use of the R package taskscheduleR.\nIn context of our institution, QC on all instruments is generally done by 10:30 AM. Consequently, we scedule automated transfer of the QC data to begin at 10:30 AM. When it works, we get all the data transferred from all instruments without any extra effort at the designated time daily, making updating of the dashboard simple.\nUnfortunately, task scheduler opens a black terminal window pop up when it is actively running. This doesn’t affect anything on the cytometer acquisition, but it can startle anyone who doesn’t expect it. In our experience, when an user is on the instrument at the time, they will either panic and immediately close the black terminal window, or after not closing for 10 seconds get annoyed and close it down. At which point, no data will be automatically transferred to the GitHub repository for that day barring manual staff intervention. Emails to users and warning note on insturment computer help reduce the occasions this above scenario occurs, but user community awareness and buy in is still required. Alternately, schedule during non-operation hours, and have the dashboard data be delayed from real time.\nTo begin, we will open the TaskSchedules.qmd file within InstrumentQC. You will first call library for the TaskScheduleR package. Then you will verify the file.path to the instrument R script is correctly updated. You will then modify the first chunk of code to desired task schedule name, and set the time. Then run that code chunk. Finally check using the command to verify the task. If you want to remove a scheduled task, run the third set commands including the previous task schedule name and if confirmation received, the previously scheduled task is now removed.\n\n\nHybrid\nIn our setting, we have automated QC set to run at 10:30, staggered by two minutes for each instruments to avoid discrepancy issues of version control when the scripts push pull data from the github repository. We also have a staff.qmd file so that manual sending of the data can occur if SRL finish QC early or an user-shutdown occurs. To avoid having both things happen, the staff.qmd file generates a Flag.csv after manual completion. Within the individual R script, the first condition checks for presence of this file, and if found doesn’t proceed with automated QC for the designated time.\nWe then set another Task Schedule for later that same day to remove the Flag.csv file, allowing a reset of the process so that automated QC can proceed the next morning. To do this, we would go into Examples_Flag.qmd and copy the example into a new .R file we call FlagRemoval.R (also covered by the .gitignore file to avoid filepath conflicts across instruments). We then add a new taskschedule to run this R script later in the afternoon.\nFinally, in attempt to reduce the time spent by the automated processing of the data during the morning being interrupted by the user, we set a TaskSchedule to pull from the GitHub repository at 6 AM, which brings in the updates from the other instruments and dashboard website, reducing the time the black terminal screen is open during the scheduled 10:30 round.\n\n\nRendering the Dashboard\nRegardless whether you implemtent the manual, automated or hybrid approach, you will end up with the updated QC data from each instrument present on the GitHub repository. These changes are tracked by git as part of version control. We previously discussed the individual instrument pages designated by their .qmd files, but we need to discuss the home/summary index.qmd page that combines all the data sources.\nBriefly, it shows the indicator blocks for the individual instruments (color-coded pass, caution, fail). The second column shows the results of the daily QC for the day (with tabsets for each instrument). And finally a six-month QC for all instruments is visible in the third column.\nAdditionally, the navbar and footer elements are specified within the _quarto.yaml file allowing for customization/rearranging/renaming as desired. In our dashboard there are additional tabs that reference Data to get the data as .csv or .pdf by the individual users, and a Help page for general information about the dashboard.\nTo assemble the dashboard with the updated data, one can either pull in the data to the local computer, and then render the project to assemble the website and individual dashboard pages into a single unit. All the changes to the files would then be committed, with the data passed to GitHub repository.\nOn the GitHub repository side, one would customize the repostitory to use GitHub pages, set to display contents of the docs folder (containing the rendered html pages). Upon pushing the updates, GitHub would then process them and display them as a website.\n\n\nFuture Directions - Github Actions\nRendering locally takes computer time and computer space. Additionally, it means each instrument needs to bring in edited versions of the website locally every morning with their initial pull. We are currently working to set up a GitHub actions that would ensure the website portion remains only within the GitHub repository (not rendered or copied locally to each instrument taking up additional memory) but additionally render at a given time reducing the requirement to manually render and push the updated dashboard. This occurs thanks to cloud access available to GitHub repositories that are public, with a certain ammount of free server time allocated to each.\nThe reason not currently available, is that it is more complicated technically and I am busy writing. But more broadly, the cloud instance needs to install R and R packages needed to run everything each and every time. And the Luciernaga package is still MB heavy due to it being in testing phase and having a lot of .fcs files in it’s extdata folder. Consequently, an area that has promise, but not yet implemented, stay tuned."
  },
  {
    "objectID": "GitAndRstudio.html",
    "href": "GitAndRstudio.html",
    "title": "Forking the project",
    "section": "",
    "text": "Once your GitHub account is set up, it’s time to use it. Our core’s version of the dashboard is contained within the InstrumentQC repository. This is publicly available, and since the software repository is licensed under a free copyleft license, you are able to fork (ie. copy) the existing project and modify it.\nTo do so, you will first navigate to the InstrumentQC repository. From here you will select the fork the repository option\n\n\n\n\n\nGitHub will then give you the option to rename the project or to keep the existing name. If you modify the name, there may be a couple additional lines of code you will need to also adjust in the future, but this will be minor enough of a concern so don’t let that stop you if you hate the existing name.\n\n\n\n\n\nWith that done, you now have your own copy of the repository. Since it is forked, you can now modify and customize the dashboard so that is customized to your instruments and tracks their QC data. Before proceeding, please make a note about your forked repositories url as you will need it later when when bringing in the project to the local computer environment with Rstudio in a little bit."
  },
  {
    "objectID": "GitAndRstudio.html#setup",
    "href": "GitAndRstudio.html#setup",
    "title": "Forking the project",
    "section": "Setup",
    "text": "Setup\nThe next big task is to set up Rstudio, making sure that Git is set up properly and that you can send the version changes to the repository files to GitHub without any issues.\nGo ahead and open up Rstudio. If it is your first time doing so, select the default R installation from the first pop-up window. Open up Rstudio for the first time, and set to use the default R installation.\nWe first need to make sure Rstudio can communicate with GitHub through Git. To do this we will first install the R package devtools. To do so, copy the following lines of code invidivually into the console window and hit enter to run the commands:\n\ninstall.packages(\"devtools\")\ninstall.packages(\"BiocManager\")\n\nlibrary(devtools)\nlibrary(BiocManager)\n\nFor coding-beginners, please note, if any errors pop up during the installation of devtools, read the red troubleshooting explanations shown, and install any missing package dependencies by swapping in the package name between the quotation marks similar to what was done in the code chunk above to install the devtools package.\nOnce devtools is installed, and you have called it via library, we can now continue.\nAdjacent to your console tab on the lower left, there is another tab called terminal. Go ahead and click it.\n\n\n\n\n\nNow that you have switched from the console to the terminal, copy-paste using your mouse/right-click the following lines of code individually, editing in your GitHub UserName, and email linked to your GitHub account:\n\ngit config --global user.email \"JohnDoe@gmail.com\"\n\ngit config --global user.name \"John Doe\""
  },
  {
    "objectID": "GitAndRstudio.html#github-token",
    "href": "GitAndRstudio.html#github-token",
    "title": "Forking the project",
    "section": "GitHub Token",
    "text": "GitHub Token\nWith this done, it is now time to get a GitHub Token that will be used for authorization for your local computer to send/receive files from your GitHub repository.\nTo do this, open a browser, and navigate back to your GitHub account, click on your profile icon on the far upper right, and then select settings\n\n\n\n\n\nFrom here, you will navigate to the lower left side and click on developer settings\n\n\n\n\n\nOnce you are on the next page, you will select Tokens (classic) option\n\n\n\n\n\nFrom there, you will now proceed to click on Generate new token and select the classic option\n\n\n\n\n\nOn the next screen, things get busy. Go ahead and place a note for the token with the instrument name so you know which match with each other. Set the expiration for no expiration or a long-period, and only click on the repo option to grant those accesses. Proceed down to the bottom of the screen and click on the green generate token button.\n\n\n\n\n\nThe website will refresh and provide you a GitHub token and the option to copy it. Copy it and temporarily store it in a .txt file (notepad) as you will need it when setting up the connection between Github and Rstudio. You will not be able to see the code again from this screen, so stash wisely, and be cautious not to store this anywhere that others may find it."
  },
  {
    "objectID": "GitAndRstudio.html#rstudio",
    "href": "GitAndRstudio.html#rstudio",
    "title": "Forking the project",
    "section": "Rstudio",
    "text": "Rstudio\nNow that you have your token, go back to Rstudio, and enter the following lines of code into your console:\n\ngitcreds::gitcreds_set()\n\nA pop-up window will appear. Follow the instructions and when prompted, provide it the Github Token code that you generated. Next hit enter. You should be all set to now pull/push (ie. receive/send) files to GitHub from your local computer.\nWhile we are here, let’s address the last thing we will need to do with the GitHub access token for. Go ahead and enter the following line of code in the console:\n\nusethis::edit_r_environ()\n\nThis will open an .Renviron file that will likely be blank. Enter the following line of code, swapping in your specific token in its entirety between the quotation marks.\n\nGITHUB_PAT &lt;- \"GitHubTokenGoesHere\"\n\nOnce this is done, save and close out of Rstudio. Open it again to make sure the changes are saved."
  },
  {
    "objectID": "Installation.html",
    "href": "Installation.html",
    "title": "Install Software",
    "section": "",
    "text": "You will first need to make sure that R, Rstudio, Rtools, Quarto and Git are installed on every instrument computer that you will be collecting daily QC data from. Follow along below for instructions for downloading each on a Windows computer.\n\n\nR is a free software and programming language used by researchers and data scientist worldwide. To begin you will need to navigate to the main website. You will first select Download R for Windows\n\n\n\n\n\nYou will be redirected to the next screen, where you should select install R for the first time:\n\n\n\n\n\nAnd finally you will see the following screen, where you will select the current version of Download R for Windows:\n\n\n\n\n\nThe next screen will ask where you want to save the installer. I generally place it on the desktop. Once downloaded, double click and proceed with the software installation, selecting the default options.\n\n\n\nRstudio is an integrated development environment (IDE), providing an interface with R that is friendlier to many users. We will use it in our context to set up project folders that will contain the code and data needed to process the QC data and export it to the dashboard.\nTo download, we first navigate to the website and select Download R Studio Desktop for Windows\n\n\n\n\n\nThis will then proceed to show the pop-up asking where you want to save the installer. Save to the desktop, and then double click the installer. Follow the default installation prompts.\n\n\n\nR packages are made up of functions that carry out specific tasks. Some of the R packages that we will be using require compilation from source code, which requires installation of Rtools to mediate this process.\nTo begin, navigate to the website and select the most recent version of Rtools\n\n\n\n\n\nThen, you will select the regular Rtools installer\n\n\n\n\n\nThis will then provide the pop-up asking where to save the installer. Place on the desktop, then after it has finished downloading, double click to run the installer. Select the default options.\n\n\n\nThe dashboard (and this website you are currently reading) are built with Quarto. It facilitates making websites from various programming languages commonly used by data scientist who didn’t start off as computer programmers. In our context, we will use it to produce both the website and individual dashboard pages.\nTo begin, after navigating to the website we will first select the Get Started tab\n\n\n\n\n\nThen we will select Download Quarto Cli to download the most recent version for Windows.\n\n\n\n\n\nFinally, the pop-up asking where we want to save the installer will pop up. Save to the desktop, and after it finished downloading, double click and select the default options.\n\n\n\nGit is used for version control by many programmers. We will be using it in the context of the dashboard for managing the processed data, and forwarding it on to GitHub for use in the dashboard.\nTo begin, we will first navigate to the website and select the download from Windows option.\n\n\n\n\n\nWe will then proceed and select install 64-bit Git for Windows Setup option\n\n\n\n\n\nFinally, the pop-up will appear asking where to save the installer. Select and save to the Desktop. After the installer has finished downloading, double click, and accept the default options. Be advised, Git has a lot of options, for now, just accept all defaults without wandering off on a “What is Vim?!?” rabbit-hole."
  },
  {
    "objectID": "Installation.html#r",
    "href": "Installation.html#r",
    "title": "Install Software",
    "section": "",
    "text": "R is a free software and programming language used by researchers and data scientist worldwide. To begin you will need to navigate to the main website. You will first select Download R for Windows\n\n\n\n\n\nYou will be redirected to the next screen, where you should select install R for the first time:\n\n\n\n\n\nAnd finally you will see the following screen, where you will select the current version of Download R for Windows:\n\n\n\n\n\nThe next screen will ask where you want to save the installer. I generally place it on the desktop. Once downloaded, double click and proceed with the software installation, selecting the default options."
  },
  {
    "objectID": "Installation.html#rstudio",
    "href": "Installation.html#rstudio",
    "title": "Install Software",
    "section": "",
    "text": "Rstudio is an integrated development environment (IDE), providing an interface with R that is friendlier to many users. We will use it in our context to set up project folders that will contain the code and data needed to process the QC data and export it to the dashboard.\nTo download, we first navigate to the website and select Download R Studio Desktop for Windows\n\n\n\n\n\nThis will then proceed to show the pop-up asking where you want to save the installer. Save to the desktop, and then double click the installer. Follow the default installation prompts."
  },
  {
    "objectID": "Installation.html#rtools",
    "href": "Installation.html#rtools",
    "title": "Install Software",
    "section": "",
    "text": "R packages are made up of functions that carry out specific tasks. Some of the R packages that we will be using require compilation from source code, which requires installation of Rtools to mediate this process.\nTo begin, navigate to the website and select the most recent version of Rtools\n\n\n\n\n\nThen, you will select the regular Rtools installer\n\n\n\n\n\nThis will then provide the pop-up asking where to save the installer. Place on the desktop, then after it has finished downloading, double click to run the installer. Select the default options."
  },
  {
    "objectID": "Installation.html#quarto",
    "href": "Installation.html#quarto",
    "title": "Install Software",
    "section": "",
    "text": "The dashboard (and this website you are currently reading) are built with Quarto. It facilitates making websites from various programming languages commonly used by data scientist who didn’t start off as computer programmers. In our context, we will use it to produce both the website and individual dashboard pages.\nTo begin, after navigating to the website we will first select the Get Started tab\n\n\n\n\n\nThen we will select Download Quarto Cli to download the most recent version for Windows.\n\n\n\n\n\nFinally, the pop-up asking where we want to save the installer will pop up. Save to the desktop, and after it finished downloading, double click and select the default options."
  },
  {
    "objectID": "Installation.html#git",
    "href": "Installation.html#git",
    "title": "Install Software",
    "section": "",
    "text": "Git is used for version control by many programmers. We will be using it in the context of the dashboard for managing the processed data, and forwarding it on to GitHub for use in the dashboard.\nTo begin, we will first navigate to the website and select the download from Windows option.\n\n\n\n\n\nWe will then proceed and select install 64-bit Git for Windows Setup option\n\n\n\n\n\nFinally, the pop-up will appear asking where to save the installer. Select and save to the Desktop. After the installer has finished downloading, double click, and accept the default options. Be advised, Git has a lot of options, for now, just accept all defaults without wandering off on a “What is Vim?!?” rabbit-hole."
  }
]