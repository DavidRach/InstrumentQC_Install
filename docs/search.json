[
  {
    "objectID": "GitAndRstudio.html",
    "href": "GitAndRstudio.html",
    "title": "Forking the project",
    "section": "",
    "text": "Now that your GitHub account is set up, it’s time to put it to use. Our core’s version of the website is contained within the InstrumentQC repository. This is publicly available, and since the software repository is licensed under a free copyleft license, you are able to fork (ie. copy) the existing project, modify it, and share (ie. distribute) it.\nTo get started, you will first navigate to the InstrumentQC repository. From here you will select the fork the repository option\n\n\n\n\n\nGitHub will then give you the option to rename the project or to keep the existing name. If you modify the name, you will need to adjust a couple of lines in the future for the file.path arguments, but this will be a minor inconvenience so don’t let that stop you if you have thought of a better name.\n\n\n\n\n\nWith that done, you now have your own copy of the repository. Since it is forked, you can now modify and customize the dashboard for the tracking of QC data for your own. Please make a note of your forked repositories url before proceeding as you will need it when bringing the project into your local computer environment with Rstudio later.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Setting up Git and Rstudio"
    ]
  },
  {
    "objectID": "GitAndRstudio.html#setup",
    "href": "GitAndRstudio.html#setup",
    "title": "Forking the project",
    "section": "Setup",
    "text": "Setup\nLet’s start by opening Rstudio. If it is your first time opening it, you will get a pop-up window asking which version of R to use. Select the newest (or default) R installation.\nWe want to make sure make sure Rstudio can communicate with GitHub through Git. To do this we will first install the R package devtools, followed by the BiocManager package. To do so, copy the following lines of code invidivually into the console window (generally on the bottom left of your screen) and hit enter to run the commands:\n\ninstall.packages(\"devtools\")\ninstall.packages(\"BiocManager\")\n\nFor coding-beginners, please note, during the installation of an R package, if you are missing required dependencies, you will be asked whether you want to install the missing packages. Select yes for these. When there are newer versions of an R package, it will ask if you want to update to the newer version, which in general is a good idea but not required if you are short on time.\nDuring the installation process, if an error is encountered, you will get an error message and a red troubleshooting explanation describing the issue. Read this carefully, and install any missing package dependencies needed to fix the issue by swapping in the package name between the quotation marks similar to what was done in the code chunk above to install the devtools package.\nOnce the R packages have succesfully installed, we need to activate them for that session by calling them with the library function before continuing.\n\nlibrary(devtools)\n\nLoading required package: usethis\n\nlibrary(BiocManager)\n\nBioconductor version '3.19' is out-of-date; the current release version '3.20'\n  is available with R version '4.4'; see https://bioconductor.org/install\n\n\n\nAttaching package: 'BiocManager'\n\n\nThe following object is masked from 'package:devtools':\n\n    install\n\n\nAdjacent to your console tab on the lower left, there is another tab called terminal. Go ahead and click it.\n\n\n\n\n\nNow that you have switched from the console to the terminal,using your mouse copy then right-click-paste the following lines of code individually into the terminal, editing the text to include your GitHub username and then email linked used for that GitHub account.\n\ngit config --global user.email \"JohnDoe@gmail.com\"\n\ngit config --global user.name \"John Doe\"",
    "crumbs": [
      "Home",
      "Getting Started",
      "Setting up Git and Rstudio"
    ]
  },
  {
    "objectID": "GitAndRstudio.html#github-token",
    "href": "GitAndRstudio.html#github-token",
    "title": "Forking the project",
    "section": "GitHub Token",
    "text": "GitHub Token\nWith this done, it is now time to get an authorization GitHub Token that will allow your local computer to send/receive files from your the GitHub repository.\nTo do this, open a browser, and navigate back to your GitHub account, click on your profile icon on the far upper right, and then select settings\n\n\n\n\n\nFrom here, you will navigate to the lower left side and click on developer settings\n\n\n\n\n\nOnce you are on the next page, you will select Tokens (classic) option\n\n\n\n\n\nFrom there, you will now proceed to click on Generate new token and select the classic option\n\n\n\n\n\nOn the next screen, things get busy. There are a few things we need to focus on. First write a note for the token containing the individual instrument name. Select for Expiration Date either the no-expiration date (to avoid needing to repeat this setup process in the near future). From here, you only need to click on the repo option to grant those associated accesses. Proceed all the way down to the bottom of the screen, and click on the green generate token button.\n\n\n\n\n\nThe website will refresh and provide you a GitHub token and the option to copy it. Copy it and temporarily store it in a .txt file (notepad) as you will need it when setting up the connection between Github and Rstudio. Please note, you will not be able to see the token code again after leaving this screen, so stash it wisely, otherwise you will need to regenerate another token. Also, make sure to be cautious and not post the token .txt file on the public internet, unless you enjoy emails from IT.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Setting up Git and Rstudio"
    ]
  },
  {
    "objectID": "GitAndRstudio.html#rstudio",
    "href": "GitAndRstudio.html#rstudio",
    "title": "Forking the project",
    "section": "Rstudio",
    "text": "Rstudio\nNow that you have your token, go back to Rstudio, and enter the following lines of code into your console:\n\ngitcreds::gitcreds_set()\n\nA pop-up window will appear. Follow the instructions and when prompted, provide it the Github Token code that you generated. Next hit enter. You should be all set to now pull/push (ie. receive/send) files to GitHub from your local computer.\nWhile we are here, let’s address the last thing we will need the GitHub access token for. Go ahead and enter the following line of code in the console:\n\nusethis::edit_r_environ()\n\nThis will open an .Renviron file that will likely be blank. Enter the following line of code, swapping in your specific token in its entirety between the quotation marks.\n\nGITHUB_PAT &lt;- \"GitHubTokenGoesHere\"\n\nOnce this is done, save the file and close-out/restart Rstudio.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Setting up Git and Rstudio"
    ]
  },
  {
    "objectID": "Installation.html",
    "href": "Installation.html",
    "title": "Install Software",
    "section": "",
    "text": "You will first need to make sure that R, Rstudio, Rtools, Quarto and Git are installed on every instrument computer that you will be collecting daily QC data from. Follow along below for instructions for downloading each on a Windows computer.\n\n\nR is a free software and programming language used by researchers and data scientist worldwide. To begin you will need to navigate to the main website. You will first select Download R for Windows\n\n\n\n\n\nYou will be redirected to the next screen, where you should select install R for the first time:\n\n\n\n\n\nAnd finally you will see the following screen, where you will select the current version of Download R for Windows:\n\n\n\n\n\nThe next screen will ask where you want to save the installer. I generally place it on the desktop. Once downloaded, double click and proceed with the software installation, selecting the default options.\n\n\n\nRstudio is an integrated development environment (IDE), providing an interface with R that is friendlier to many users. We will use it in our context to set up project folders that will contain the code and data needed to process the QC data and export it to the dashboard.\nTo download, we first navigate to the website and select Download R Studio Desktop for Windows\n\n\n\n\n\nThis will then proceed to show the pop-up asking where you want to save the installer. Save to the desktop, and then double click the installer. Follow the default installation prompts.\n\n\n\nR packages are made up of functions that carry out specific tasks. Some of the R packages that we will be using require compilation from source code, which requires installation of Rtools to mediate this process.\nTo begin, navigate to the website and select the most recent version of Rtools\n\n\n\n\n\nThen, you will select the regular Rtools installer\n\n\n\n\n\nThis will then provide the pop-up asking where to save the installer. Place on the desktop, then after it has finished downloading, double click to run the installer. Select the default options.\n\n\n\nThe dashboard (and this website you are currently reading) are built with Quarto. It facilitates making websites from various programming languages commonly used by data scientist who didn’t start off as computer programmers. In our context, we will use it to produce both the website and individual dashboard pages.\nTo begin, after navigating to the website we will first select the Get Started tab\n\n\n\n\n\nThen we will select Download Quarto Cli to download the most recent version for Windows.\n\n\n\n\n\nFinally, the pop-up asking where we want to save the installer will pop up. Save to the desktop, and after it finished downloading, double click and select the default options.\n\n\n\nGit is used for version control by many programmers. We will be using it in the context of the dashboard for managing the processed data, and forwarding it on to GitHub for use in the dashboard.\nTo begin, we will first navigate to the website and select the download from Windows option.\n\n\n\n\n\nWe will then proceed and select install 64-bit Git for Windows Setup option\n\n\n\n\n\nFinally, the pop-up will appear asking where to save the installer. Select and save to the Desktop. After the installer has finished downloading, double click, and accept the default options. Be advised, Git has a lot of options, for now, just accept all defaults without wandering off on a “What is Vim?!?” rabbit-hole.",
    "crumbs": [
      "Home",
      "Getting Started"
    ]
  },
  {
    "objectID": "Installation.html#r",
    "href": "Installation.html#r",
    "title": "Install Software",
    "section": "",
    "text": "R is a free software and programming language used by researchers and data scientist worldwide. To begin you will need to navigate to the main website. You will first select Download R for Windows\n\n\n\n\n\nYou will be redirected to the next screen, where you should select install R for the first time:\n\n\n\n\n\nAnd finally you will see the following screen, where you will select the current version of Download R for Windows:\n\n\n\n\n\nThe next screen will ask where you want to save the installer. I generally place it on the desktop. Once downloaded, double click and proceed with the software installation, selecting the default options.",
    "crumbs": [
      "Home",
      "Getting Started"
    ]
  },
  {
    "objectID": "Installation.html#rstudio",
    "href": "Installation.html#rstudio",
    "title": "Install Software",
    "section": "",
    "text": "Rstudio is an integrated development environment (IDE), providing an interface with R that is friendlier to many users. We will use it in our context to set up project folders that will contain the code and data needed to process the QC data and export it to the dashboard.\nTo download, we first navigate to the website and select Download R Studio Desktop for Windows\n\n\n\n\n\nThis will then proceed to show the pop-up asking where you want to save the installer. Save to the desktop, and then double click the installer. Follow the default installation prompts.",
    "crumbs": [
      "Home",
      "Getting Started"
    ]
  },
  {
    "objectID": "Installation.html#rtools",
    "href": "Installation.html#rtools",
    "title": "Install Software",
    "section": "",
    "text": "R packages are made up of functions that carry out specific tasks. Some of the R packages that we will be using require compilation from source code, which requires installation of Rtools to mediate this process.\nTo begin, navigate to the website and select the most recent version of Rtools\n\n\n\n\n\nThen, you will select the regular Rtools installer\n\n\n\n\n\nThis will then provide the pop-up asking where to save the installer. Place on the desktop, then after it has finished downloading, double click to run the installer. Select the default options.",
    "crumbs": [
      "Home",
      "Getting Started"
    ]
  },
  {
    "objectID": "Installation.html#quarto",
    "href": "Installation.html#quarto",
    "title": "Install Software",
    "section": "",
    "text": "The dashboard (and this website you are currently reading) are built with Quarto. It facilitates making websites from various programming languages commonly used by data scientist who didn’t start off as computer programmers. In our context, we will use it to produce both the website and individual dashboard pages.\nTo begin, after navigating to the website we will first select the Get Started tab\n\n\n\n\n\nThen we will select Download Quarto Cli to download the most recent version for Windows.\n\n\n\n\n\nFinally, the pop-up asking where we want to save the installer will pop up. Save to the desktop, and after it finished downloading, double click and select the default options.",
    "crumbs": [
      "Home",
      "Getting Started"
    ]
  },
  {
    "objectID": "Installation.html#git",
    "href": "Installation.html#git",
    "title": "Install Software",
    "section": "",
    "text": "Git is used for version control by many programmers. We will be using it in the context of the dashboard for managing the processed data, and forwarding it on to GitHub for use in the dashboard.\nTo begin, we will first navigate to the website and select the download from Windows option.\n\n\n\n\n\nWe will then proceed and select install 64-bit Git for Windows Setup option\n\n\n\n\n\nFinally, the pop-up will appear asking where to save the installer. Select and save to the Desktop. After the installer has finished downloading, double click, and accept the default options. Be advised, Git has a lot of options, for now, just accept all defaults without wandering off on a “What is Vim?!?” rabbit-hole.",
    "crumbs": [
      "Home",
      "Getting Started"
    ]
  },
  {
    "objectID": "InstrumentLayout.html",
    "href": "InstrumentLayout.html",
    "title": "Dashboard Overview",
    "section": "",
    "text": "Having processed your Instrument QC data for that instrument, when you check the data/instrument/archive folder you will now find both archive data .csv files corresponding to the data for both Gain and MFI tracking. We are now ready to discuss how the dashboard elements are coded and assemble to produce the individual webpages.\nFor our overall dashboard layout, the home/summary page is specified by the code contained within the index.qmd file. It’s main purpose is to serve as a landing page that rapidly loads in (since it doesn’t have any interactive elements).\nThen, each instrument has its own interactive webpage for exploring its data (taking longer to load as the data exploration is fully interactive). Each instruments webpage is specified by code contained within the individual instrument .qmd files (Aurora3L.qmd, Aurora4L.qmd, Aurora5L.qmd, AuroraCS.qmd). These share similar code layouts, but mainly differ in that 1) the file paths reference different archive data folders corresponding to their respective instruments; and 2) given each instrument has different laser-configurations, those with UV and Yellow-Green Lasers will have more plot elements than the others.\nFor this walk-though, we will be examining the the Aurora5L.qmd file from the original repository.",
    "crumbs": [
      "Home",
      "Customization",
      "Building a Instrument Webpage"
    ]
  },
  {
    "objectID": "InstrumentLayout.html#retrieving-data",
    "href": "InstrumentLayout.html#retrieving-data",
    "title": "Dashboard Overview",
    "section": "Retrieving Data",
    "text": "Retrieving Data\nHaving set up the file.path (MainFolder) and specified the particular instrument folder (“5L” in this case) the next two code blocks retrieve the data in the Gain and MFI .csv files, and then filter the data for the last twelve months. If you wanted to modify the time period shown, you would edit the code block at this point to increase/decrease the range.\n\nMFI_5L &lt;- Luciernaga:::CurrentData(x=\"5L\", MainFolder=MainFolder, type = \"MFI\")\nGain_5L &lt;- Luciernaga:::CurrentData(x=\"5L\", MainFolder=MainFolder, type = \"Gain\")\n\n\nWindowOfInterest &lt;- Sys.time() - months(12)\n\nMFI_5L &lt;- MFI_5L %&gt;% filter(DateTime &gt;= WindowOfInterest)\nGain_5L &lt;- Gain_5L %&gt;% filter(DateTime &gt;= WindowOfInterest)\n\nThe following code block references the .csv file containing Field-Service Engineer Visits, which are depicted as red vertical dashed lines in the .pdf version of the plots that can be exported.\n\nData &lt;- read.csv(\"AuroraMaintenance.csv\", check.names=FALSE)\n\nData &lt;- Data %&gt;% filter(!str_detect(reason, \"lean\"))\n\nRepair5L &lt;- Data %&gt;% filter(instrument %in% \"5L\")",
    "crumbs": [
      "Home",
      "Customization",
      "Building a Instrument Webpage"
    ]
  },
  {
    "objectID": "InstrumentLayout.html#processing-the-data",
    "href": "InstrumentLayout.html#processing-the-data",
    "title": "Dashboard Overview",
    "section": "Processing the Data",
    "text": "Processing the Data\nThe next three code chunks are rather large, each representing three types of data that will make up the three columns seen on each instruments page (MFI, Gain, RCV). We will work through the first code chunk, which is MFI.\n\nx &lt;- MFI_5L\nx &lt;- x %&gt;% dplyr::filter(Timepoint %in% c(\"Before\", \"After\"))\nTheColumns &lt;- x %&gt;% select(where(~is.numeric(.)||is.integer(.))) %&gt;% colnames()\nTheColumns &lt;- setdiff(TheColumns, \"TIME\")\nTheIntermediate &lt;- TheColumns[!str_detect(TheColumns, \"Gain\")]\nTheColumnNames &lt;- TheIntermediate[str_detect(TheIntermediate, \"-A\")]\n  \nUltraVioletGains &lt;- TheColumnNames[str_detect(TheColumnNames, \"^UV\")]\nVioletGains &lt;- TheColumnNames[str_detect(TheColumnNames, \"^V\")]\nBlueGains &lt;- TheColumnNames[str_detect(TheColumnNames, \"^B\")]\nYellowGreenGains &lt;- TheColumnNames[str_detect(TheColumnNames, \"^YG\")]\nRedGains &lt;- TheColumnNames[str_detect(TheColumnNames, \"^R\")]\n\nScatterGains &lt;- TheIntermediate[str_detect(TheIntermediate, \"SC-\")]\nScatterGains &lt;- Luciernaga:::ScalePriority(ScatterGains)\nLaserGains &lt;- TheIntermediate[str_detect(TheIntermediate, \"Laser\")]\nLaserGains &lt;- Luciernaga:::ColorPriority(LaserGains)\nScalingGains &lt;- TheIntermediate[str_detect(TheIntermediate, \"Scaling\")]\nScalingGains &lt;- Luciernaga:::ColorPriority(ScalingGains)\nOtherGains &lt;- c(ScatterGains, LaserGains, ScalingGains)\n\nThe retrieved data for MFI goes through a couple processing steps to retrieve the column names present within the .csv file. From there it removes those that show Gain as they will be plotted along with RCV separately. Once this is done, it filters the column names by presence of string characters in their names to separate the list of colnames into shorter list based on laser, scatter, etc.\nAt this point, each of the above element is simply smaller list of column names, that will then be plotted and visualized together. This happens in the portion of the larger code chunk shown below, with the smaller list being provided to the MeasurementType arguments.\n\nUltraVioletPlotsMFI &lt;- QC_Plots(x=x, FailedFlag=TRUE, MeasurementType=UltraVioletGains,\n                      plotType = \"comparison\", returntype = \"plots\",\n                      Metadata=\"Timepoint\", strict = TRUE, YAxisLabel = \"MFI\",\n                      RepairVisits=Repair5L)\n\nVioletPlotsMFI &lt;- QC_Plots(x=x, FailedFlag=TRUE, MeasurementType=VioletGains,\n                      plotType = \"comparison\", returntype = \"plots\",\n                      Metadata=\"Timepoint\", strict = TRUE, YAxisLabel = \"MFI\",\n                      RepairVisits=Repair5L)\n\nBluePlotsMFI &lt;- QC_Plots(x=x, FailedFlag=TRUE, MeasurementType=BlueGains,\n                      plotType = \"comparison\", returntype = \"plots\",\n                      Metadata=\"Timepoint\", strict = TRUE, YAxisLabel = \"MFI\",\n                      RepairVisits=Repair5L)\n\nYellowGreenPlotsMFI &lt;- QC_Plots(x=x, FailedFlag=TRUE, MeasurementType=YellowGreenGains,\n                      plotType = \"comparison\", returntype = \"plots\",\n                      Metadata=\"Timepoint\", strict = TRUE, YAxisLabel = \"MFI\",\n                      RepairVisits=Repair5L)\n\nRedPlotsMFI &lt;- QC_Plots(x=x, FailedFlag=TRUE, MeasurementType=RedGains,\n                     plotType = \"comparison\", returntype = \"plots\",\n                     Metadata=\"Timepoint\", strict = TRUE, YAxisLabel = \"MFI\",\n                     RepairVisits=Repair5L)\n\nScatterPlotsMFI &lt;- QC_Plots(x=x, FailedFlag=TRUE, MeasurementType=ScatterGains,\n                     plotType = \"comparison\", returntype = \"plots\",\n                     Metadata=\"Timepoint\", strict = TRUE, YAxisLabel = \" \",\n                     RepairVisits=Repair5L)\n\nLaserPlotsMFI &lt;- QC_Plots(x=x, FailedFlag=TRUE, MeasurementType=LaserGains,\n                     plotType = \"comparison\", returntype = \"plots\",\n                     Metadata=\"Timepoint\", strict = TRUE, YAxisLabel = \" \",\n                     RepairVisits=Repair5L)\n\nOnce this is done, each of the elements above contains a list of plots corresponding to the column names that were provided in the smaller list. These will be referenced later when building the dashboard in the desired layout.\nThis process is then repeated again in two large code chunks for both Gain and RCV, with some small differences in how the column names are separated into smaller list, and how the individual ggplots are generated.\nStarting on line 205 of Aurora5L.qmd we have the following code-chunk:\n\nPDFPlots &lt;- c(UltraVioletPlotsMFI, VioletPlotsMFI, BluePlotsMFI, YellowGreenPlotsMFI, RedPlotsMFI, LaserPlotsMFI, ScatterPlotsMFI, UltraVioletPlotsGain, VioletPlotsGain, BluePlotsGain, YellowGreenPlotsGain, RedPlotsGain, ScatterPlotsGain, LaserDelayPlotsGain, LaserPowerPlotsGain,  ScalingPlotsGain, UltraVioletPlotsRCV, VioletPlotsRCV, BluePlotsRCV, YellowGreenPlotsRCV, RedPlotsRCV, ScatterPlotsRCV)\n\nFilename &lt;- paste0(\"QCPlots_5L\")\n\nPDF &lt;- Utility_Patchwork(x=PDFPlots, filename=Filename, returntype=\"pdf\", outfolder=MainFolder, thecolumns=1)\n\nThis code chunk above assembled all the plots we generated in the section above, and saves them as the .pdf for the individual instrument that can be seen under the data tab of the dashboard website.",
    "crumbs": [
      "Home",
      "Customization",
      "Building a Instrument Webpage"
    ]
  },
  {
    "objectID": "InstrumentLayout.html#visualizing-the-data",
    "href": "InstrumentLayout.html#visualizing-the-data",
    "title": "Dashboard Overview",
    "section": "Visualizing the Data",
    "text": "Visualizing the Data\n\nMFI\nNow that the instrument data has been assembled into plots for the individual measurement types and lasers, it is time to display the plots in a way that produces the instrument page visible on the original dashboard website.\nAs mentioned at the beginning of this page, the orientation of this dashboard is set to columns. Consequently, on line 218 you will see the following “## MFI {.tabset}”. The two # designate the first column. All plots visualized until the next ## (“## Gain {.tabset}” in our case at line 345) will consequently be present within this first column.\nIf we desired to change the ordering MFI column last rather and first, we would move everything from “## MFI {.tabset}” until “## Gain {.tabset}” and shift to desired order position.\nThe presence of “{.tabset}” designates that within this colum (denoted by the ##), there will be multiple tab options to switch between. The first is seen in the following code-chunk:\n\nggplotly(UltraVioletPlotsMFI[[1]])\nggplotly(UltraVioletPlotsMFI[[2]])\n# etc...\n\nThe tab name visible on the website is denoted by the title: argument. Within the code block itself, the individual plots are made interactive using the plotly function ggplotly, calling the specific detector plot sequentially.\nThis is then repeated on the next tabs for Violet, Blue, Yellow-Green and Red detectors. Then Scatter, LaserPower, LaserDelay, LaserScatter plots are visualized in their respective tabs.\nFinally, marking the end of the first column we find a card element “{.card title=”MFI”}” that appears within the tabset at the end but contains no plots to serve as a reference of what the first column plots are showing.\n\n\nGain and rCV\nAbove we walked through the process of displaying the MFI plots generated in the first section within tabsets for individual lasers. At line 345, we encounter “## Gain {.tabset}” which designates the second column of the dashboard, containing tabs for the Gain plots by laser. The overall layout is similar to what we encountered, differing here and there based on additional plots present within it’s respective archive .csv but not found in the bead csv MFI derrived from.\nAfter all the Gain plots are plotted, we encounter the third column “## RCV {.tabset}” at line 472, continuing until the end.",
    "crumbs": [
      "Home",
      "Customization",
      "Building a Instrument Webpage"
    ]
  },
  {
    "objectID": "InstrumentLayout.html#summary",
    "href": "InstrumentLayout.html#summary",
    "title": "Dashboard Overview",
    "section": "Summary",
    "text": "Summary\nThe individual instrument .qmd file is setup to retrieve the instrument specific archive data and filter for a desired time range. Once this is done, the individual column names in the .csv are identified and split on the basis of characters in their names into smaller list (typically by laser). This process is then repeated for each measurement type (MFI, Gain, RCV). For the assembly of the actual dashboard itself, the ordering of the measurement type columns is denoted by the ## as encountered, with the tab-sets within each of these also displayed by the order encountered. By rearranging the order, modifications to the individual instrument dashboard pages can be customized.\nIf you have multiple instruments, you would modify each instruments .qmd file in a similar way to ensure the correct archive data folder is being referenced, and then customize which plots you want displayed when and where.",
    "crumbs": [
      "Home",
      "Customization",
      "Building a Instrument Webpage"
    ]
  },
  {
    "objectID": "repositoryelements.html",
    "href": "repositoryelements.html",
    "title": "Introduction",
    "section": "",
    "text": "In the previous section, we installed the required software, set up the Git permissions for the individual local computers needed for Rstudio and GitHub to communicate and pass along updated files stored locally to your remote repository, and made sure the required R packages were installed.\nIn this section, we will now start to modify existing R code originally configured for the the UMGCC flow core computers and adapt it for your setup. This involves first understanding the individual components that are present within the InstrumentQC repository and their purpose. Afterwards, we will identify and change file.paths within the individual .R scripts and .qmd files to find the newly acquired QC .fcs files (and DailyQC report .csv files for Cytek instruments) in their respective folders on your local computers.",
    "crumbs": [
      "Home",
      "Customization",
      "Setting File Paths"
    ]
  },
  {
    "objectID": "repositoryelements.html#qmd-files",
    "href": "repositoryelements.html#qmd-files",
    "title": "Introduction",
    "section": "qmd files",
    "text": "qmd files\nWithin the files currently visible, we have individual files that are named after invidual instruments (Aurora3L, Aurora4L, Aurora5L, AuroraCS, LSRII, Aria, Canto). Those that have a .qmd ending contain both text and code segments that together produce a website page. For example, when the Aurora5L.qmd document gets processed into .html, it forms the following webpage. By contrast, when the LSRII.qmd document gets processed into .html, it forms its own webpage.\nEach Instrument.qmd file, by combining and rearranging individual building code block elements within, allows for extensive modification to account for for the differences for the various instruments (number of lasers, detector configuration and parameters) that can be seen on their respective webpages. We will examine these .qmd files and their individual elements at greater depth in the Instrument Layout section.",
    "crumbs": [
      "Home",
      "Customization",
      "Setting File Paths"
    ]
  },
  {
    "objectID": "repositoryelements.html#r-files",
    "href": "repositoryelements.html#r-files",
    "title": "Introduction",
    "section": "R files",
    "text": "R files\nContinuing scrolling down, we can identify another repeating element set of files (TheScript_3L, TheScript_4L, TheScript_5L, TheScript_CS) that end in .R.\nThese are .R files, and they contain only R code. In this context, they work as scripts, containing sets of instructions to find the correct storage folder, identify any new data, process it, and save it to individual instruments Archive data folder. Our first modification of file.paths will occur within these Script files.",
    "crumbs": [
      "Home",
      "Customization",
      "Setting File Paths"
    ]
  },
  {
    "objectID": "repositoryelements.html#individual-instrument-folders",
    "href": "repositoryelements.html#individual-instrument-folders",
    "title": "Introduction",
    "section": "Individual Instrument Folders",
    "text": "Individual Instrument Folders\nWe will touch on the other files present within this initial view later on. Before moving to adjust the file paths, go ahead and double click on the data folder. Within this folder, you can see individual folders corresponding to the individual instruments, as well as some of the .pdf and .csv files. Go ahead and double click on the 5L folder.\nAt this point, you see nothing but an Archive folder. If however, you were to look in the middle of the processing step, you would see a bunch of .csv and .fcs files also present, since this folder is where the individual computer copies all identified newly acquired files before they are processed. Once they are successfully processed, their data gets copied to the .csv files found within the Archive folder. Once this has been done successfully, the copies of the unprocessed data located in this folder get deleted, leaving only the Archive folder with the processed data present.",
    "crumbs": [
      "Home",
      "Customization",
      "Setting File Paths"
    ]
  },
  {
    "objectID": "repositoryelements.html#further-considerations",
    "href": "repositoryelements.html#further-considerations",
    "title": "Introduction",
    "section": "Further considerations",
    "text": "Further considerations\nNow that you have an initial idea of the contents of the repository, you can start to prepare a to-do list for changes that you will need to carry out for adapting the existing project to your own instruments.\nIf you have only a single cytometer, you will only need a single Instrument.R, Instrument.qmd and Instrument folder to process and store the data, and ultimately generate a single tab webpage.\nIf you have 8 instruments, you would need to add those three elements for each one of them, modifying the names accordingly.\nAnd for each instrument, the file.path arguments will need to direct the computer to find the newly acquired QC data in the correct folder location on those computers, and then pass it to the corresponding Instrument folder so that it can be processed.\nIf this sounds confusing, do not worry, we will go through these indivual elements in the process of adjusting the file.paths.",
    "crumbs": [
      "Home",
      "Customization",
      "Setting File Paths"
    ]
  },
  {
    "objectID": "rpackages.html",
    "href": "rpackages.html",
    "title": "Introduction",
    "section": "",
    "text": "Previously, we installed the required software, and set up the Git permissions on individual local computers needed for Rstudio and GitHub to communicate and pass along updated files stored locally to your remote repository. We will next install the R packages needed for data processing and handling of the QC data generated daily.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Installing R packages"
    ]
  },
  {
    "objectID": "rpackages.html#cran",
    "href": "rpackages.html#cran",
    "title": "Introduction",
    "section": "CRAN",
    "text": "CRAN\nWe will start by first installing some of the R packages we will need that are found on the CRAN repository.\nTo do so, we will first run the following code chunk:\n\ninstall.packages(\"dplyr\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"devtools\")\ninstall.packages(\"BiocManager\")\n\nWhen installing a package, we surround the package name in quotation marks. Forgetting to add the quotation marks is a common source of an error message when first getting started in R that we all have done at some point.\nFor the above, we installed the following R packages: dplyr, which allows rearranging of data’s rows and columns; ggplot2 used in creating the visualization plots. And finally we installed BiocManager, which is the R package that serves as the manager for installation of packages located in Bioconductor repositories. We will use it’s functions within the next section.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Installing R packages"
    ]
  },
  {
    "objectID": "rpackages.html#bioconductor",
    "href": "rpackages.html#bioconductor",
    "title": "Introduction",
    "section": "Bioconductor",
    "text": "Bioconductor\nBioconductor is a repository for R packages that specialize in bioinformatics. The majority of Cytometry R packages can be found here. To be able to install Bioconductor packages, we first need to load the BiocManager package at the start of a session through the library call.\n\nlibrary(BiocManager)\n\nNote, unlike with install.packages() function where we surround the name of the R package name in quotation marks, when loading a package with the library() function quotation marks are not needed.\nNow that BiocManager is active, we can use it to install packages from Bioconductor with the following code:\n\ninstall(\"flowCore\")\ninstall(\"flowWorkspace\")\ninstall(\"ggcyto\")\n\nWe installed flowCore and flowWorkspace, which provide the core infrastructure needed to work with .fcs files in R. The ggcyto package is used to visualize the .fcs data.\nAnd finally, we need to install one package from GitHub using devtools. This is our R package, Luciernaga, where I have added functions that are needed to process the QC files and assemble the dashboard. We are in the process of submitting the package to Bioconductor, but until then, it is available via GitHub.\n\nlibrary(devtools)\ninstall_github(\"https://github.com/DavidRach/Luciernaga\", dependencies = TRUE)\n\nNote, Luciernaga contains a config.win file needed for a communicating with Python function that throws an error on some of our campus computers causing the IT firewall to trigger. Since it is not needed for building the website, if you encounter this error, install the following “branch” of the package that doesn’t contain that function.\n\nlibrary(devtools)\ninstall_github(\"https://github.com/DavidRach/Luciernaga\", ref=\"NoPythonConfig\", dependencies = TRUE)",
    "crumbs": [
      "Home",
      "Getting Started",
      "Installing R packages"
    ]
  },
  {
    "objectID": "rpackages.html#troubleshooting",
    "href": "rpackages.html#troubleshooting",
    "title": "Introduction",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nFor coding-beginners, please note, during the installation of an R package, if you are missing required dependencies, you will be asked whether you want to install the missing packages. Select yes for these. When there are newer versions of an R package, it will ask if you want to update to the newer version, which in general is a good idea but not required if you are short on time.\nDuring the installation process, if an error is encountered, you will get an error message and a red troubleshooting explanation describing the issue. Read this carefully, and search online for the missing dependency, identifying whether it is a CRAN or a Bioconductor package. Then use the corresponding installation setup as seen above to install the missing package and hopefully fix the issue. Once this is done, we can attempt to install the R package that had failed to install.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Installing R packages"
    ]
  },
  {
    "objectID": "rpackages.html#summary",
    "href": "rpackages.html#summary",
    "title": "Introduction",
    "section": "Summary",
    "text": "Summary\nHaving completed the steps above, you should now have the main R packages needed to run the R code to process the instument QC data and assemble the Quarto dashboard. Congratulations on making progress!\nIf you are installing on multiple computers and encounter dependency packages that throw errors, that you then need to type in to install, please open an issue and let me know so that I can update the list above accordingly.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Installing R packages"
    ]
  },
  {
    "objectID": "Automation.html",
    "href": "Automation.html",
    "title": "Manual Transfer",
    "section": "",
    "text": "Once the file.paths to find the instruments archive data are found, and the individual instrument R script and .qmd file are updated to reflect these, it is possible to start rendering the individual instrument webpage on the local computer. But when you have multiple instruments contributing data to the dashboard home page and individual instrument pages, we need a way to ensure the data from each instrument gets transferred to GitHub to allow Quarto to render the update version. This can be done either manually, or in an automated fashion. We will explore both options\n\nManual Transfer\nWe will first navigate to the InstrumentQC folder and locate the Examples_staff.qmd file. We will duplicate it, creating a new copy that will simply be called staff.qmd. This name is set within the .gitignore file to be ignored, allowing local copies with different file paths to be present on each instrument computer without causing issues in the version control.\nOnce we open the duplicated and renamed staff.qmd file, we find a single code chunk with several lines of code. This code chunk is meant to be executed by staff after morning QC has been carried out on the instrument by clicking the green play button showing “Run Current Chunk” on the upper-left side of the code-block. Once this is done, it will run the code, find the file.path to the .R script for that individual instrument, and run it. In the process, it executes the pull from the GitHub repository, processes the new data, and then pushes the updated data to the GitHub repository.\nOnce the source command has finished executing, the option is present to introduce a Flag.csv file. This will prevent automated transfer if set-up from occuring later that day (more below).\nManual sending the data has advantages as you are already at the instrument during the Instrument QC process, and the extra minute to run the current code-chunk shouldn’t add that much extra inconvenience. The drawbacks occur in that if you or others are not comfortable with Rstudio, you need to interact with it, and not forget to send the data before an user gets on the instrument.\n\n\nAutomating Transfer\nThe second option is to set up an automated transfer of the QC data from each instrument to the GitHub repository. This is possible by using the Windows Task Scheduler, which is the same system that schedules when your computer updates run, etc. The time at which the automated transfer is carried out can be designated by the user, and setup is carried out through use of the R package taskscheduleR.\nIn context of our institution, QC on all instruments is generally done by 10:30 AM. Consequently, we scedule automated transfer of the QC data to begin at 10:30 AM. When it works, we get all the data transferred from all instruments without any extra effort at the designated time daily, making updating of the dashboard simple.\nUnfortunately, task scheduler opens a black terminal window pop up when it is actively running. This doesn’t affect anything on the cytometer acquisition, but it can startle anyone who doesn’t expect it. In our experience, when an user is on the instrument at the time, they will either panic and immediately close the black terminal window, or after not closing for 10 seconds get annoyed and close it down. At which point, no data will be automatically transferred to the GitHub repository for that day barring manual staff intervention. Emails to users and warning note on insturment computer help reduce the occasions this above scenario occurs, but user community awareness and buy in is still required. Alternately, schedule during non-operation hours, and have the dashboard data be delayed from real time.\nTo begin, we will open the TaskSchedules.qmd file within InstrumentQC. You will first call library for the TaskScheduleR package. Then you will verify the file.path to the instrument R script is correctly updated. You will then modify the first chunk of code to desired task schedule name, and set the time. Then run that code chunk. Finally check using the command to verify the task. If you want to remove a scheduled task, run the third set commands including the previous task schedule name and if confirmation received, the previously scheduled task is now removed.\n\n\nHybrid\nIn our setting, we have automated QC set to run at 10:30, staggered by two minutes for each instruments to avoid discrepancy issues of version control when the scripts push pull data from the github repository. We also have a staff.qmd file so that manual sending of the data can occur if SRL finish QC early or an user-shutdown occurs. To avoid having both things happen, the staff.qmd file generates a Flag.csv after manual completion. Within the individual R script, the first condition checks for presence of this file, and if found doesn’t proceed with automated QC for the designated time.\nWe then set another Task Schedule for later that same day to remove the Flag.csv file, allowing a reset of the process so that automated QC can proceed the next morning. To do this, we would go into Examples_Flag.qmd and copy the example into a new .R file we call FlagRemoval.R (also covered by the .gitignore file to avoid filepath conflicts across instruments). We then add a new taskschedule to run this R script later in the afternoon.\nFinally, in attempt to reduce the time spent by the automated processing of the data during the morning being interrupted by the user, we set a TaskSchedule to pull from the GitHub repository at 6 AM, which brings in the updates from the other instruments and dashboard website, reducing the time the black terminal screen is open during the scheduled 10:30 round.\n\n\nRendering the Dashboard\nRegardless whether you implemtent the manual, automated or hybrid approach, you will end up with the updated QC data from each instrument present on the GitHub repository. These changes are tracked by git as part of version control. We previously discussed the individual instrument pages designated by their .qmd files, but we need to discuss the home/summary index.qmd page that combines all the data sources.\nBriefly, it shows the indicator blocks for the individual instruments (color-coded pass, caution, fail). The second column shows the results of the daily QC for the day (with tabsets for each instrument). And finally a six-month QC for all instruments is visible in the third column.\nAdditionally, the navbar and footer elements are specified within the _quarto.yaml file allowing for customization/rearranging/renaming as desired. In our dashboard there are additional tabs that reference Data to get the data as .csv or .pdf by the individual users, and a Help page for general information about the dashboard.\nTo assemble the dashboard with the updated data, one can either pull in the data to the local computer, and then render the project to assemble the website and individual dashboard pages into a single unit. All the changes to the files would then be committed, with the data passed to GitHub repository.\nOn the GitHub repository side, one would customize the repostitory to use GitHub pages, set to display contents of the docs folder (containing the rendered html pages). Upon pushing the updates, GitHub would then process them and display them as a website.\n\n\nFuture Directions - Github Actions\nRendering locally takes computer time and computer space. Additionally, it means each instrument needs to bring in edited versions of the website locally every morning with their initial pull. We are currently working to set up a GitHub actions that would ensure the website portion remains only within the GitHub repository (not rendered or copied locally to each instrument taking up additional memory) but additionally render at a given time reducing the requirement to manually render and push the updated dashboard. This occurs thanks to cloud access available to GitHub repositories that are public, with a certain ammount of free server time allocated to each.\nThe reason not currently available, is that it is more complicated technically and I am busy writing. But more broadly, the cloud instance needs to install R and R packages needed to run everything each and every time. And the Luciernaga package is still MB heavy due to it being in testing phase and having a lot of .fcs files in it’s extdata folder. Consequently, an area that has promise, but not yet implemented, stay tuned.",
    "crumbs": [
      "Home",
      "Customization",
      "Automation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "InstrumentQC",
    "section": "",
    "text": "InstrumentQC\nHaving reached this webpage, you likely discovered the InstrumentQC website I implemented at the University of Maryland, Baltimore to track daily QC for the Cytek and BD instruments. I am providing this documentation for those of you who would like to understand how it is implemented, whether for curiosity or because you’d like to modify the code to set up something similar for your own cytometers.\nHaving placed significant effort into this pet-project, I would love if others in the cytometery community could also benefit. Since assembling everything from scratch requires some familiarity with both R and Git, I am writing up this detailed explanation of the process behind setting up and customizing the dashboards to give the average cytometry aficionado a fighting-chance.\nIf you get stuck, or parts of the documentation are unclear, please reach out by opening an issue and I will try my best to help you troubleshoot. What makes free-and-open source software great (and in my opinion,fun to work with) is seeing how the small collective contributions of users improve the projects over time.\nBest- David\n\n\nHigh-level Overview\nAfter daily QC is run on a Cytek instrument, the relevant information is stored as .fcs and .csv files in specific folders. Using functions incorporated in the Luciernaga R package, the data associated with these newly generated files is processed using R once a day at a user-designated time (implemented via Windows Task Sceduler). The version control software Git keeps track of changes to the processed data, passing updates to the online GitHub repository for storage. This data is then referenced when building the dashboard website using Quarto, which is published as a GitHub page allowing for url access.\nFor multiple instruments, the above process is repeated on each instruments computer, and the Quarto webpages are modified to display data from each instrument."
  },
  {
    "objectID": "filepaths.html",
    "href": "filepaths.html",
    "title": "File Paths",
    "section": "",
    "text": "As mentioned in the introduction, within the .R and .qmd files are lines of code containing file.paths. These serve as addresses used by the software to find folders containing the necessary daily QC files on your individual computer. Since installation of SpectroFlo (and its associated files) will differ on your computer compared to ours, you will need to ensure to update these file paths so that your computer can retrieve the daily QC files from the correct folder when prompted to do so by the executed R scripts.\n\n\nFor this section, we will focus on data, as it is where new QC files get copied to, processed and stored, and subsequently used in assembly of the dashboard website.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s now start by locating your SpectroFlo associated folders you will need to have the file.paths for when modifying the R scripts. On our computer, when the instruments were set up, these were present within the local C: drive folder\n\n\n\n\n\nYou can notice there are two SpectroFlo associated folders present here, Cytekbio and Cytekbioexport. When we retrieve a file.path, we would right click the &gt; symbol. Then we would select the copy address as text option\n When we paste, we get something like this: C:\\Cytekbio. The  designates a hierarchy of folders descending down to the target folder or file. When working in R, we can reference this by taking the individual folder names and linking them with the file.path function. For this example, it would look something like the following for C:\\Cytekbio :\n\nPathToTheFolderOfInterest &lt;- file.path(\"C:\", \"CytekbioExport\")\nPathToTheFolderOfInterest\n\n[1] \"C:/CytekbioExport\"\n\n\nOne of the reasons file.path only needs the folder names is \\ vs / will vary depending on what operating system your computer is running on (Windows, Mac or Linux). Additionally, R requires Windows paths that when we right-click copy are shown as \\ be switched to /. Using file.path is easier in the long run than needing to remember which is permitted and which one is not.\nAdditionally, extra elements can be appended to the file.path by simply including the existing path variable, and then adding the next folder in between ““. For example:\n\nNextFolderDown &lt;- file.path(PathToTheFolderOfInterest, \"Experiments\")\nNextFolderDown\n\n[1] \"C:/CytekbioExport/Experiments\"\n\n\nWe can see that this is the next folder under, so let’s double click and find the file structure.\n We can see in this case that we have folders for Experiments, FCSFiles and Setup. Additionally there are files for individual dates for SetupEngineLog, AppLoginLog and ExperimentUnmixingLog that track activity previously carried out in SpectroFlo. We will navigate to the next folder “Setup”\n\n\n\n\nSetupFolderPath &lt;- file.path(NextFolderDown, \"Setup\")\nSetupFolderPath\n\n[1] \"C:/CytekbioExport/Experiments/Setup\"\n\n\n\n\n\n\n\nIt is within this folder that the DailyQC reports are stored as .csv files. Opening one, we can see their overall structure resembles something like this. The individual elements are beyond the scope of this tutorial, but the data from the majority gets extracted out and utilized for the dashboard.\nUnfortunately, the .csv does not follow a “tidy” format (having gaps in spaces and rows rather than equally filled rectangular space). Consequently, a bunch of functions are used to process the data behind the scenes until it is returned in a “tidy” format that R can work with (these can later be downloaded from the Data tab on the dashboard).\nIt’s this path (SetupFolderPath) that we will need to provide to the Rscript so that it can locate new DailyQC .csv files and copy them over to the InstrumentQC folder. Make a note of this path before we continue.\n\n\n\nIf we are relying on the .fcs files acquired during the process of DailyQC, we would navigate from this folder down one additional level.\n\nQCFCSFilePath &lt;- file.path(SetupFolderPath, \"DailyQC\")\nQCFCSFilePath\n\n[1] \"C:/CytekbioExport/Experiments/Setup/DailyQC\"\n\n\nIt is here where we find the DailyQCDataSample .fcs files.\n\n\n\n\n\n\n\n\n\n\nWe would consequently provide this file.path to the Rscript to provision the files needed to calculate the MFI parameters.\n\n\n\nAt our institution, we separately using the the same QC beads used for Daily QC a 3000 bead before and after .fcs sample to compare the changes in MFI after the QC has adjusted. Within SpectroFlo, these are acquired under the Admin account, organized within an experiment corresponding to the month. As a result, these are stored with the other .fcs files acquired by all users that exist within SpectroFlo folder while they wait to be exported as zipped folders.\nBecause this can take up quite a bit of memory space in context of a core facility, for our particular SpectroFlo setup, these folders are found under an external hard-drive.\nWe would consequently start exploring the folders (and setting up a file.path) like this:\n\nExternal &lt;- file.path(\"D:\")\nExternal\n\n[1] \"D:\"\n\n\n\n\n\n\n\n\nFCSFiles &lt;- file.path(External, \"Aurora 5 FCS_Files\")\nFCSFiles\n\n[1] \"D:/Aurora 5 FCS_Files\"\n\n\n\n\n\n\n\n\nExperiments &lt;- file.path(FCSFiles, \"Experiments\")\nExperiments\n\n[1] \"D:/Aurora 5 FCS_Files/Experiments\"\n\n\n\n\n\n\n\n\nAdmin &lt;- file.path(Experiments, \"Admin\")\nAdmin\n\n[1] \"D:/Aurora 5 FCS_Files/Experiments/Admin\"\n\n\n And finally, the important thing is to note the structure that each experiment file name takes. In our case for this instrument, the folders are set to QC_2024-11. When we are having the Rscript find the new QC files for the given day, it calls the function System.time to return the date and time. These are broken into month and day.\n\nSys.time()\n\n[1] \"2025-02-12 14:24:52 EST\"\n\n\nConsequently, at the level of this folder, it would look for a folder named “QC_2024-” with the corresponding month provided by Sys.time call. R recognizes character strings exactly in this case, so if you had a name mismatch (ex. “QC 2024-” or “QC_2024_”) it would fail to find the correct folder and search the contents within. So this is an area to be aware of and adjust the code accordingly for how you structure the name.",
    "crumbs": [
      "Home",
      "Customization"
    ]
  },
  {
    "objectID": "filepaths.html#identifying-file-path",
    "href": "filepaths.html#identifying-file-path",
    "title": "File Paths",
    "section": "",
    "text": "For this section, we will focus on data, as it is where new QC files get copied to, processed and stored, and subsequently used in assembly of the dashboard website.",
    "crumbs": [
      "Home",
      "Customization"
    ]
  },
  {
    "objectID": "filepaths.html#finding-required-file-paths",
    "href": "filepaths.html#finding-required-file-paths",
    "title": "File Paths",
    "section": "",
    "text": "Let’s now start by locating your SpectroFlo associated folders you will need to have the file.paths for when modifying the R scripts. On our computer, when the instruments were set up, these were present within the local C: drive folder\n\n\n\n\n\nYou can notice there are two SpectroFlo associated folders present here, Cytekbio and Cytekbioexport. When we retrieve a file.path, we would right click the &gt; symbol. Then we would select the copy address as text option\n When we paste, we get something like this: C:\\Cytekbio. The  designates a hierarchy of folders descending down to the target folder or file. When working in R, we can reference this by taking the individual folder names and linking them with the file.path function. For this example, it would look something like the following for C:\\Cytekbio :\n\nPathToTheFolderOfInterest &lt;- file.path(\"C:\", \"CytekbioExport\")\nPathToTheFolderOfInterest\n\n[1] \"C:/CytekbioExport\"\n\n\nOne of the reasons file.path only needs the folder names is \\ vs / will vary depending on what operating system your computer is running on (Windows, Mac or Linux). Additionally, R requires Windows paths that when we right-click copy are shown as \\ be switched to /. Using file.path is easier in the long run than needing to remember which is permitted and which one is not.\nAdditionally, extra elements can be appended to the file.path by simply including the existing path variable, and then adding the next folder in between ““. For example:\n\nNextFolderDown &lt;- file.path(PathToTheFolderOfInterest, \"Experiments\")\nNextFolderDown\n\n[1] \"C:/CytekbioExport/Experiments\"\n\n\nWe can see that this is the next folder under, so let’s double click and find the file structure.\n We can see in this case that we have folders for Experiments, FCSFiles and Setup. Additionally there are files for individual dates for SetupEngineLog, AppLoginLog and ExperimentUnmixingLog that track activity previously carried out in SpectroFlo. We will navigate to the next folder “Setup”",
    "crumbs": [
      "Home",
      "Customization"
    ]
  },
  {
    "objectID": "filepaths.html#daily-qc-files",
    "href": "filepaths.html#daily-qc-files",
    "title": "File Paths",
    "section": "",
    "text": "SetupFolderPath &lt;- file.path(NextFolderDown, \"Setup\")\nSetupFolderPath\n\n[1] \"C:/CytekbioExport/Experiments/Setup\"\n\n\n\n\n\n\n\nIt is within this folder that the DailyQC reports are stored as .csv files. Opening one, we can see their overall structure resembles something like this. The individual elements are beyond the scope of this tutorial, but the data from the majority gets extracted out and utilized for the dashboard.\nUnfortunately, the .csv does not follow a “tidy” format (having gaps in spaces and rows rather than equally filled rectangular space). Consequently, a bunch of functions are used to process the data behind the scenes until it is returned in a “tidy” format that R can work with (these can later be downloaded from the Data tab on the dashboard).\nIt’s this path (SetupFolderPath) that we will need to provide to the Rscript so that it can locate new DailyQC .csv files and copy them over to the InstrumentQC folder. Make a note of this path before we continue.",
    "crumbs": [
      "Home",
      "Customization"
    ]
  },
  {
    "objectID": "filepaths.html#bead-fcs-files-system",
    "href": "filepaths.html#bead-fcs-files-system",
    "title": "File Paths",
    "section": "",
    "text": "If we are relying on the .fcs files acquired during the process of DailyQC, we would navigate from this folder down one additional level.\n\nQCFCSFilePath &lt;- file.path(SetupFolderPath, \"DailyQC\")\nQCFCSFilePath\n\n[1] \"C:/CytekbioExport/Experiments/Setup/DailyQC\"\n\n\nIt is here where we find the DailyQCDataSample .fcs files.\n\n\n\n\n\n\n\n\n\n\nWe would consequently provide this file.path to the Rscript to provision the files needed to calculate the MFI parameters.",
    "crumbs": [
      "Home",
      "Customization"
    ]
  },
  {
    "objectID": "filepaths.html#bead-fcs-files-system-1",
    "href": "filepaths.html#bead-fcs-files-system-1",
    "title": "File Paths",
    "section": "",
    "text": "At our institution, we separately using the the same QC beads used for Daily QC a 3000 bead before and after .fcs sample to compare the changes in MFI after the QC has adjusted. Within SpectroFlo, these are acquired under the Admin account, organized within an experiment corresponding to the month. As a result, these are stored with the other .fcs files acquired by all users that exist within SpectroFlo folder while they wait to be exported as zipped folders.\nBecause this can take up quite a bit of memory space in context of a core facility, for our particular SpectroFlo setup, these folders are found under an external hard-drive.\nWe would consequently start exploring the folders (and setting up a file.path) like this:\n\nExternal &lt;- file.path(\"D:\")\nExternal\n\n[1] \"D:\"\n\n\n\n\n\n\n\n\nFCSFiles &lt;- file.path(External, \"Aurora 5 FCS_Files\")\nFCSFiles\n\n[1] \"D:/Aurora 5 FCS_Files\"\n\n\n\n\n\n\n\n\nExperiments &lt;- file.path(FCSFiles, \"Experiments\")\nExperiments\n\n[1] \"D:/Aurora 5 FCS_Files/Experiments\"\n\n\n\n\n\n\n\n\nAdmin &lt;- file.path(Experiments, \"Admin\")\nAdmin\n\n[1] \"D:/Aurora 5 FCS_Files/Experiments/Admin\"\n\n\n And finally, the important thing is to note the structure that each experiment file name takes. In our case for this instrument, the folders are set to QC_2024-11. When we are having the Rscript find the new QC files for the given day, it calls the function System.time to return the date and time. These are broken into month and day.\n\nSys.time()\n\n[1] \"2025-02-12 14:24:52 EST\"\n\n\nConsequently, at the level of this folder, it would look for a folder named “QC_2024-” with the corresponding month provided by Sys.time call. R recognizes character strings exactly in this case, so if you had a name mismatch (ex. “QC 2024-” or “QC_2024_”) it would fail to find the correct folder and search the contents within. So this is an area to be aware of and adjust the code accordingly for how you structure the name.",
    "crumbs": [
      "Home",
      "Customization"
    ]
  }
]