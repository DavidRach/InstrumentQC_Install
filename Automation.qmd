---
toc: true
---

Previously, we identified and set the file.paths to the location where the .fcs (and .csv) files are stored after daily QC. We also identified the file.paths to the respective instrument and archive folders within your InstrumentQC folder. Afterwards, we modified the Instrument.R file to reflect these paths, and successfully copied over and processed the existing data into an Archive.csv file. 

We will now move on to discuss how to set up manual and automated options to facilitate this process as part of a daily routine.We will also discuss the steps involved in the export of the processed data to the GitHub repository for use in creating the website, and how to orchestrate this when you have multiple instruments sending QC data. 


# Setting up Manual Transfer

Within your forked InstrumentQC folder, you can find an "Examples_staff.qmd" file. This is a .qmd file, that contains within the individual chunks code specific to each instrument that get used when setting up a staff.qmd file for use in manually transferring data from individual instruments. 

Start by first duplicating the "Examples_staff.qmd" and rename it as "staff.qmd". This file name is ignored by Git (through its inclusion in the .gitignore file) allowing for slightly different versions of this file on each instrument to exist without the different file.paths causing version control issues.

Now that you have the staff.qmd file, go ahead and open it. You will encounter a large code chunk containing several lines of code. On the upper-right hand side, you will see a green play button that says "Run Current Chunk".

When this is clicked, the code-block will run, provide the correct file paths and trigger Rscript you have modified to process the newly acquired QC data and sending it to GitHub.

As long as your file.paths were correctly set up, the only thing staff needs to manually do in the morning to generate the processed data is the following steps:

- Open Rstudio.
- Make sure they are in the InstrumentQC project folder
- Open the staff.qmd file
- Hit the Run Code Chunk button
- And that's it.

This manual sending of data approach is useful if you have fewer instruments and are already present at the instrument at the time of QC, as the extra minute to hit "Run Code Chunk" doesn't add that much an inconvenience. Drawbacks are if you forget, the website when rendered will not contain the data for that day, and you will need to go back to the instrument and send the data. 

# Setting Up Automated Transfer

Another option is to set up an automated transfer of the QC data for each instrument to the GitHub repository at a specific time of the day. This is possible by utilizing the Windows Task Scheduler (the system that schedules your computer updates, etc.) via the TaskScheduleR R package. What this looks like in practice is at the designated time, a black terminal/console window will pop up on the computer screen for around a minute, run the code in the Instrument.R file, push the data to GitHub and then close the terminal window.

One of the considerations with using this approach is deciding at what time of the day you want the automated script to run and process newly acquired QC data. For our institution, QC on all instruments is historically wrapped up most days by 10:30 AM, which is when we schedule the automated tasks to begin for the first instrument. This way, we ensure the websites and dashboards have the most recent data rather than the displayed data being delayed a day due to the automated script running after QC for the day was acquired. 

One issue with the automated approach is the sudden appearance black terminal window pop up that pops up at the designated time to run the code.This doesn't affect anything on the cytometer while acquiring, but it can startle an user who is not expecting it to appear. In our experience, when an user is on the instrument at the time the console window appears, they will either panic and immediately close the black terminal window, or after it appears to not be doing anything for 10 seconds get annoyed and close it down. This results in incomplete processing of the data, with the data not being sent to server, requiring staff intervention. 

In our context, since we are trying to have the data be current and can't just set the automated processing to occur during the middle of the night, we set it between when QC is being acquired and when users usually first book. Emailing/training users to not close the terminal has had mixed results on community buy-in. We are also working on a way to provide the computer privilidges to the TaskManagethis requires community buy-in and requires continuous intervention. 

To get started, open the "TaskSchedules.qmd" file for example code. Load the respective required packages with the library function, and navigate to an individual instruments block of code. Modify it for your own instrument by adding the file.paths you identified to where new QC data is stored, and the location of your forked InstrumentQC repository that contains the individual Instrument.R scripts. 

Within the code block, there are set-up blocks for three automated task: "QC_Instrument_Morning", "FlagRemoval" and RepoPull. We will discuss each of these in turn, but let's focus on setting the QC_Instrument_Morning. 

To begin, we will open the TaskSchedules.qmd file within InstrumentQC. You will first call library for the TaskScheduleR package. Then you will verify the file.path to the instrument R script is correctly updated. You will then modify the first chunk of code to desired task schedule name, and set the time. Then run that code chunk. Finally check using the command to verify the task. If you want to remove a scheduled task, run the third set commands including the previous task schedule name and if confirmation received, the previously scheduled task is now removed. 

# Hybrid

In our setting, we have automated QC set to run at 10:30, staggered by two minutes for each instruments to avoid discrepancy issues of version control when the scripts push pull data from the github repository. We also have a staff.qmd file so that manual sending of the data can occur if SRL finish QC early or an user-shutdown occurs. To avoid having both things happen, the staff.qmd file generates a Flag.csv after manual completion. Within the individual R script, the first condition checks for presence of this file, and if found doesn't proceed with automated QC for the designated time. 

We then set another Task Schedule for later that same day to remove the Flag.csv file, allowing a reset of the process so that automated QC can proceed the next morning. To do this, we would go into Examples_Flag.qmd and copy the example into a new .R file we call FlagRemoval.R (also covered by the .gitignore file to avoid filepath conflicts across instruments). We then add a new taskschedule to run this R script later in the afternoon.

Finally, in attempt to reduce the time spent by the automated processing of the data during the morning being interrupted by the user, we set a TaskSchedule to pull from the GitHub repository at 6 AM, which brings in the updates from the other instruments and dashboard website, reducing the time the black terminal screen is open during the scheduled 10:30 round. 

Additionally, the Instrument.R script monitors whether data has already been sent to GitHub during the day by producing a Flag.CSV
Once the source command has finished executing, the option is present to introduce a Flag.csv file. This will prevent automated transfer if set-up from occuring later that day (more below).

# Rendering the Dashboard

Regardless whether you implemtent the manual, automated or hybrid approach, you will end up with the updated QC data from each instrument present on the GitHub repository. These changes are tracked by git as part of version control. We previously discussed the individual instrument pages designated by their .qmd files, but we need to discuss the home/summary index.qmd page that combines all the data sources. 

Briefly, it shows the indicator blocks for the individual instruments (color-coded pass, caution, fail). The second column shows the results of the daily QC for the day (with tabsets for each instrument). And finally a six-month QC for all instruments is visible in the third column. 

Additionally, the navbar and footer elements are specified within the _quarto.yaml file allowing for customization/rearranging/renaming as desired. In our dashboard there are additional tabs that reference Data to get the data as .csv or .pdf by the individual users, and a Help page for general information about the dashboard. 

To assemble the dashboard with the updated data, one can either pull in the data to the local computer, and then render the project to assemble the website and individual dashboard pages into a single unit. All the changes to the files would then be committed, with the data passed to GitHub repository.

On the GitHub repository side, one would customize the repostitory to use GitHub pages, set to display contents of the docs folder (containing the rendered html pages). Upon pushing the updates, GitHub would then process them and display them as a website. 

# Future Directions - Github Actions

Rendering locally takes computer time and computer space. Additionally, it means each instrument needs to bring in edited versions of the website locally every morning with their initial pull. We are currently working to set up a GitHub actions that would ensure the website portion remains only within the GitHub repository (not rendered or copied locally to each instrument taking up additional memory) but additionally render at a given time reducing the requirement to manually render and push the updated dashboard. This occurs thanks to cloud access available to GitHub repositories that are public, with a certain ammount of free server time allocated to each. 

The reason not currently available, is that it is more complicated technically and I am busy writing. But more broadly, the cloud instance needs to install R and R packages needed to run everything each and every time. And the Luciernaga package is still MB heavy due to it being in testing phase and having a lot of .fcs files in it's extdata folder. Consequently, an area that has promise, but not yet implemented, stay tuned.

